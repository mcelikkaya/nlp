{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_lemmatize.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AxPXj1itNwZ",
        "colab_type": "code",
        "outputId": "a4473c63-38db-465b-c8a3-97926949f7b5",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "source": [
        "\"\"\"\n",
        "In this notebook i want to try the effect of lemmatizing the sentences.\n",
        "I will use famous imdb sentiment analysis database.\n",
        "My theory is:\n",
        "There must be a difference of semantics according to selected sentence parts.\n",
        "I will train and test method on 3 sets\n",
        "1)Only adjective and adverbs\n",
        "2)Only verbs\n",
        "3)Verbs adjective and adverbs  \n",
        "in all cases i will exclude nouns\n",
        "The results I obtained for accuracy of models\n",
        "\n",
        "1)Only adjective and adverbs    -> Accuracy %78\n",
        "2)Only verbs                    -> Accuracy %63\n",
        "3)Verbs adjective and adverbs   -> Accuracy %79\n",
        "\n",
        "nltk takes so long,so i could not arrange all cell in correct order.\n",
        "I runned in different order but arranged like this.\n",
        "\n",
        "\"\"\"\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7317b820-515d-4525-abb7-0c40f6dd17a7\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-7317b820-515d-4525-abb7-0c40f6dd17a7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving labeledTrainData.tsv to labeledTrainData (1).tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRTtfl_YtUfN",
        "colab_type": "code",
        "outputId": "30177002-8e49-450b-d471-3f8ac22ef4f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Embedding\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obFcmTv0te-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "df = pd.read_csv(io.BytesIO(uploaded['labeledTrainData.tsv']), sep='\\t', quoting=3)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o435PkmYtg2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_review(text):\n",
        "    # Strip HTML tags\n",
        "    text = re.sub('<[^<]+?>', ' ', text)\n",
        " \n",
        "    # Strip escaped quotes\n",
        "    text = text.replace('\\\\\"', '')\n",
        " \n",
        "    # Strip quotes\n",
        "    text = text.replace('\"', '')\n",
        " \n",
        "    return text\n",
        "df['cleaned_review'] = df['review'].apply(clean_review)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XyD9WqaOrdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def get_important_features(sentence):\n",
        "    features = []\n",
        "    for w in nltk.word_tokenize(sentence):\n",
        "        pos = get_wordnet_pos(w)\n",
        "        if pos == wordnet.ADJ or pos == wordnet.VERB or pos == wordnet.ADV :\n",
        "            features.append( lemmatizer.lemmatize(w, pos) )\n",
        "    return \" \".join(features) \n",
        "\n",
        "def get_important_features_verbs(sentence):\n",
        "    features = []\n",
        "    for w in nltk.word_tokenize(sentence):\n",
        "        pos = get_wordnet_pos(w)\n",
        "        if  pos == wordnet.VERB  :\n",
        "            features.append( lemmatizer.lemmatize(w, pos) )\n",
        "    return \" \".join(features)     \n",
        "\n",
        "def get_important_features_adv_adj(sentence):\n",
        "    features = []\n",
        "    for w in nltk.word_tokenize(sentence):\n",
        "        pos = get_wordnet_pos(w)\n",
        "        if  pos == wordnet.ADJ or  pos == wordnet.ADV   :\n",
        "            features.append( lemmatizer.lemmatize(w, pos) )\n",
        "    return \" \".join(features)       \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOHhx9UjzIiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_validate(mdl,step):\n",
        "  for i in range(step):\n",
        "    mdl.fit(padded_train[:-100], y_train[:-100], epochs=3, batch_size=512, verbose=1,\n",
        "          validation_data=(padded_train[-100:], y_train[-100:]))\n",
        "    scores = mdl.evaluate(padded_test, y_test, verbose=1)\n",
        "    print(\"Accuracy:\", scores[1]) # 0.875\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVbssrUglUES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl0mDtC-O3Yy",
        "colab_type": "code",
        "outputId": "b26685f5-ab3f-4883-865b-e73ad2992853",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6Y_1pwJPBeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_reviews = df['cleaned_review'].values\n",
        "lemmatized = [get_important_features(review) for review in cleaned_reviews]\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfiKNK6rnFgn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "lemmatized_verbs = [get_important_features_verbs(review) for review in cleaned_reviews]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSFyNGIosD8k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "lemmatized_adv_adj = [get_important_features_adv_adj(review) for review in cleaned_reviews]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYmdrxfRSUMW",
        "colab_type": "code",
        "outputId": "fafe2f24-5041-40ee-bd91-6ebdb71d6722",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "get_important_features(\"go down 've start listen watch here there watch watch \")\n",
        "for index,l in enumerate(lemmatized[0:10]):\n",
        "  print(index,l)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 go down 've start listen watch here there watch watch again Maybe just get certain be really just maybe make up be guilty be remember go see be originally release have feel also obvious be bad Visually impressive be so remotely anyway then be go find consent make most say make true be really nice actual finally be only so exclude Criminal be psychopathic powerful want dead so bad be rant want know be be supply so maybe just turn whole Also have have come film usually work let alone whole perform complex be be most not then away do give ironically be be truly most talented ever be guilty Well 've give well do n't know be different close know be extremely nice stupid most be not\n",
            "1 Classic be very entertain obviously go great faithfully classic succeed do so watch appreciate be not predictable come have only Obviously different only criticize more important be entertain be most never enjoy be classic be very entertain make easy be\n",
            "2 give mutate use fossilize most deadly however high be open savagely stalk restrict be attack large pre-historical be big hardly carnivorous be real be astound terrifyingly not savagely be stalk run most Furthermore third more dangerous slow full appear special provide excite stir be quite be make totally .Middling react appropriately become give vigorously physical dodge run dangle ridiculous final deadly small realistic Other be follow much well Steven fill be badly direct take too many previous be Australian usually work many occasionally Rating average\n",
            "3 be assume praise great film ever do n't somewhere do n't do n't do n't appear Cultured swan-song unmitigated match lugubrious questionable especially be be allow anywhere very fashionably small decide be bisexual so continue sing high few be get double be see not also appear monstrously double play Good be represent scatter sometimes have such n't n't be provide Music be hard high particular possess aural add uninspired conduct paralytic unfold ritual mention record be often very slow have altogether feel orchestral modern be still superior\n",
            "4 Superbly wondrously unpretentious somewhat give false 're deal serious harrow not barely later 're up nonsensical gratuitous be orphan unusually close even slightly pervert playfully rip naked then unshaven several whole Well do do n't Sick Anyway flee nasty brutally slaughter friendly take however even raise train actual later 're face ultimate mythical incredibly valuable be coincidentally Very few ever make little narrative be Most have up even less be magnificently single be pleasingly retard go totally suddenly n't Fred big black local be Italian big hideous preposterous least there other original French Survive be uniquely make just much\n",
            "5 know be such bad get pretty good good do not Sure offensive gratuitous be not only be good like small like then see well DEAD\n",
            "6 have be very good come up short special act have look be n't so there be more have be well be link get never clearly just plod Walken have be completely be most have potential really bad make\n",
            "7 watch 'm do not have several most not related man be short prepare own instead let have say curiously show shield believe be extremely have American general be bad so be\n",
            "8 even then be grossly overprice feature big such Billy incredibly talented be take hammer tire consistently break fourth seemingly want make successful successful shamelessly name due unfathomable be presume lose get big be not dead rather lock presumably Perhaps just vomit bad\n",
            "9 be full many clear be much more\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKJlSZApSv0J",
        "colab_type": "code",
        "outputId": "a6d50b74-20fe-4e20-d2e9-c7813b6d9bb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "for index,l in enumerate(lemmatized[0:10]):\n",
        "  print(index,l)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 go 've start listen watch watch watch get certain be make be guilty be remember go see be release have feel obvious be bad impressive be be go find consent make most say make true be nice actual be exclude Criminal be psychopathic powerful want dead bad be rant want know be be supply turn whole have have come film work let whole perform complex be be most do give be be most talented be guilty 've give do know be different close know be nice stupid most be\n",
            "1 Classic be entertain go great classic succeed do watch appreciate be predictable come have different criticize important be entertain be most enjoy be classic be entertain make easy be\n",
            "2 give mutate use fossilize most high be open stalk restrict be attack large pre-historical be big carnivorous be real be astound be stalk run most third dangerous slow full appear special provide excite stir be be make .Middling react become give physical dodge run dangle ridiculous final small realistic Other be follow much fill be direct take many previous be Australian work many Rating average\n",
            "3 be assume praise great film do do do do appear Cultured swan-song unmitigated match lugubrious questionable be be allow small decide be bisexual continue sing high few be get be see appear play Good be represent scatter have such be provide Music be hard high particular possess aural add uninspired conduct paralytic unfold ritual mention record be slow have feel orchestral modern be superior\n",
            "4 unpretentious give false 're deal serious harrow 're nonsensical gratuitous be orphan pervert rip naked several whole do do Sick flee nasty slaughter take raise train actual 're face ultimate mythical valuable be few make little narrative be Most have be single be retard go Fred big black local be Italian big hideous preposterous least other original French Survive be make much\n",
            "5 know be such bad get good good do Sure offensive gratuitous be be good like small like see DEAD\n",
            "6 have be good come short special act have look be be have be be link get plod Walken have be be most have potential bad make\n",
            "7 watch 'm do have several most related man be short prepare own let have say show shield believe be have American general be bad be\n",
            "8 be overprice feature big such talented be take hammer tire break fourth want make successful successful name due unfathomable be presume lose get big be dead lock vomit bad\n",
            "9 be full many clear be much\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMe1JBz-W4P5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['lemmatized'] = lemmatized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESjPQflIsPBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "df['lemmatized_verbs'] = lemmatized_verbs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YneJyacZW_HK",
        "colab_type": "code",
        "outputId": "6cedf363-4069-489e-d96e-dd8eefeff31c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        }
      },
      "source": [
        "df['lemmatized_adv_adj'] = lemmatized_adv_adj\n",
        "\n",
        "df.head(10)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "      <th>cleaned_review</th>\n",
              "      <th>lemmatized</th>\n",
              "      <th>lemmatized_verbs</th>\n",
              "      <th>lemmatized_adv_adj</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"5814_8\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"With all this stuff going down at the moment ...</td>\n",
              "      <td>With all this stuff going down at the moment w...</td>\n",
              "      <td>go down 've start listen watch here there watc...</td>\n",
              "      <td>go 've start listen watch watch watch get be m...</td>\n",
              "      <td>down here there again Maybe just certain reall...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"2381_9\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
              "      <td>The Classic War of the Worlds by Timothy Hines...</td>\n",
              "      <td>Classic be very entertain obviously go great f...</td>\n",
              "      <td>be entertain go succeed do watch appreciate be...</td>\n",
              "      <td>Classic very obviously great faithfully classi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"7759_3\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
              "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
              "      <td>give mutate use fossilize most deadly however ...</td>\n",
              "      <td>give mutate use fossilize be open stalk restri...</td>\n",
              "      <td>most deadly however high savagely large pre-hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"3630_4\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"It must be assumed that those who praised thi...</td>\n",
              "      <td>It must be assumed that those who praised this...</td>\n",
              "      <td>be assume praise great film ever do n't somewh...</td>\n",
              "      <td>be assume praise film do do do do appear Cultu...</td>\n",
              "      <td>great ever n't somewhere n't n't n't swan-song...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"9495_8\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
              "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
              "      <td>Superbly wondrously unpretentious somewhat giv...</td>\n",
              "      <td>give 're deal harrow 're be orphan pervert rip...</td>\n",
              "      <td>Superbly wondrously unpretentious somewhat fal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>\"8196_8\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"I dont know why people think this is such a b...</td>\n",
              "      <td>I dont know why people think this is such a ba...</td>\n",
              "      <td>know be such bad get pretty good good do not S...</td>\n",
              "      <td>know be get do be be like like see</td>\n",
              "      <td>such bad pretty good good not Sure offensive g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>\"7166_2\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"This movie could have been very good, but com...</td>\n",
              "      <td>This movie could have been very good, but come...</td>\n",
              "      <td>have be very good come up short special act ha...</td>\n",
              "      <td>have be come act have look be be have be be li...</td>\n",
              "      <td>very good up short special n't so there more w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>\"10633_1\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"I watched this video at a friend's house. I'm...</td>\n",
              "      <td>I watched this video at a friend's house. I'm ...</td>\n",
              "      <td>watch 'm do not have several most not related ...</td>\n",
              "      <td>watch 'm do have man be prepare let have say s...</td>\n",
              "      <td>not several most not related short own instead...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>\"319_1\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"A friend of mine bought this film for £1, and...</td>\n",
              "      <td>A friend of mine bought this film for £1, and ...</td>\n",
              "      <td>even then be grossly overprice feature big suc...</td>\n",
              "      <td>be overprice feature talented be take hammer t...</td>\n",
              "      <td>even then grossly big such Billy incredibly co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>\"8713_10\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"&lt;br /&gt;&lt;br /&gt;This movie is full of references....</td>\n",
              "      <td>This movie is full of references. Like Mad M...</td>\n",
              "      <td>be full many clear be much more</td>\n",
              "      <td>be be</td>\n",
              "      <td>full many clear much more</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ...                                 lemmatized_adv_adj\n",
              "0   \"5814_8\"  ...  down here there again Maybe just certain reall...\n",
              "1   \"2381_9\"  ...  Classic very obviously great faithfully classi...\n",
              "2   \"7759_3\"  ...  most deadly however high savagely large pre-hi...\n",
              "3   \"3630_4\"  ...  great ever n't somewhere n't n't n't swan-song...\n",
              "4   \"9495_8\"  ...  Superbly wondrously unpretentious somewhat fal...\n",
              "5   \"8196_8\"  ...  such bad pretty good good not Sure offensive g...\n",
              "6   \"7166_2\"  ...  very good up short special n't so there more w...\n",
              "7  \"10633_1\"  ...  not several most not related short own instead...\n",
              "8    \"319_1\"  ...  even then grossly big such Billy incredibly co...\n",
              "9  \"8713_10\"  ...                          full many clear much more\n",
              "\n",
              "[10 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VJa_Bcccx9v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "d65e79c9-3e53-40b2-905b-3688c1e6183e"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Embedding\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['lemmatized_adv_adj'], df['sentiment'], test_size=0.2)\n",
        "vectorizer = CountVectorizer(binary=True, stop_words=stopwords.words('english'), lowercase=True, min_df=3, max_df=0.9, max_features=5000)\n",
        "padded_train,padded_test,vectorizer = arr_to_padded(X_train,X_test)\n",
        "MAX_SEQ_LENGHT = len(max(padded_train, key=len))\n",
        "\n",
        "model_adj = Sequential()\n",
        "model_adj.add(Embedding(len(vectorizer.get_feature_names()) + 1,\n",
        "                    64,  # Embedding size\n",
        "                    input_length=MAX_SEQ_LENGHT))\n",
        "model_adj.add(Conv1D(64, 5, activation='relu'))\n",
        "model_adj.add(MaxPooling1D(5))\n",
        "model_adj.add(Flatten())\n",
        "model_adj.add(Dense(units=64, activation='relu'))\n",
        "model_adj.add(Dense(units=1, activation='sigmoid'))\n",
        " \n",
        "model_adj.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model_adj.summary())\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 166, 64)           262528    \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 162, 64)           20544     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 32, 64)            0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                131136    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 414,273\n",
            "Trainable params: 414,273\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NnDPtt7vODM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "82ddfb39-62d6-4996-adfe-f79bc264cab7"
      },
      "source": [
        "train_validate(model_adj,3)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 19900 samples, validate on 100 samples\n",
            "Epoch 1/3\n",
            "19900/19900 [==============================] - 12s 612us/step - loss: 0.6758 - accuracy: 0.5971 - val_loss: 0.5679 - val_accuracy: 0.7700\n",
            "Epoch 2/3\n",
            "19900/19900 [==============================] - 12s 603us/step - loss: 0.4844 - accuracy: 0.7759 - val_loss: 0.3929 - val_accuracy: 0.8700\n",
            "Epoch 3/3\n",
            "19900/19900 [==============================] - 12s 607us/step - loss: 0.4035 - accuracy: 0.8181 - val_loss: 0.3732 - val_accuracy: 0.8300\n",
            "5000/5000 [==============================] - 1s 250us/step\n",
            "Accuracy: 0.8015999794006348\n",
            "Train on 19900 samples, validate on 100 samples\n",
            "Epoch 1/3\n",
            "19900/19900 [==============================] - 12s 606us/step - loss: 0.3684 - accuracy: 0.8387 - val_loss: 0.3638 - val_accuracy: 0.8400\n",
            "Epoch 2/3\n",
            "19900/19900 [==============================] - 12s 603us/step - loss: 0.3418 - accuracy: 0.8525 - val_loss: 0.3710 - val_accuracy: 0.8400\n",
            "Epoch 3/3\n",
            "19900/19900 [==============================] - 12s 617us/step - loss: 0.3162 - accuracy: 0.8654 - val_loss: 0.3949 - val_accuracy: 0.8600\n",
            "5000/5000 [==============================] - 1s 272us/step\n",
            "Accuracy: 0.7973999977111816\n",
            "Train on 19900 samples, validate on 100 samples\n",
            "Epoch 1/3\n",
            "19900/19900 [==============================] - 12s 607us/step - loss: 0.2877 - accuracy: 0.8795 - val_loss: 0.3969 - val_accuracy: 0.8500\n",
            "Epoch 2/3\n",
            "19900/19900 [==============================] - 12s 608us/step - loss: 0.2569 - accuracy: 0.8964 - val_loss: 0.4083 - val_accuracy: 0.8600\n",
            "Epoch 3/3\n",
            "19900/19900 [==============================] - 12s 607us/step - loss: 0.2237 - accuracy: 0.9121 - val_loss: 0.4341 - val_accuracy: 0.8800\n",
            "5000/5000 [==============================] - 1s 252us/step\n",
            "Accuracy: 0.7838000059127808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIIsFEfjqN2J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "b846c10a-e6c9-4912-a588-a3669bfb42a4"
      },
      "source": [
        "train_validate(model_adj,3)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 19900 samples, validate on 100 samples\n",
            "Epoch 1/3\n",
            "19900/19900 [==============================] - 12s 601us/step - loss: 0.1869 - accuracy: 0.9309 - val_loss: 0.4800 - val_accuracy: 0.8700\n",
            "Epoch 2/3\n",
            "19900/19900 [==============================] - 12s 603us/step - loss: 0.1589 - accuracy: 0.9427 - val_loss: 0.5020 - val_accuracy: 0.8700\n",
            "Epoch 3/3\n",
            "19900/19900 [==============================] - 12s 603us/step - loss: 0.1280 - accuracy: 0.9565 - val_loss: 0.5553 - val_accuracy: 0.8300\n",
            "5000/5000 [==============================] - 1s 243us/step\n",
            "Accuracy: 0.770799994468689\n",
            "Train on 19900 samples, validate on 100 samples\n",
            "Epoch 1/3\n",
            "19900/19900 [==============================] - 12s 598us/step - loss: 0.1035 - accuracy: 0.9673 - val_loss: 0.6189 - val_accuracy: 0.8300\n",
            "Epoch 2/3\n",
            "19900/19900 [==============================] - 12s 600us/step - loss: 0.0838 - accuracy: 0.9728 - val_loss: 0.6771 - val_accuracy: 0.8100\n",
            "Epoch 3/3\n",
            "19900/19900 [==============================] - 12s 600us/step - loss: 0.0679 - accuracy: 0.9797 - val_loss: 0.7465 - val_accuracy: 0.8100\n",
            "5000/5000 [==============================] - 1s 248us/step\n",
            "Accuracy: 0.76419997215271\n",
            "Train on 19900 samples, validate on 100 samples\n",
            "Epoch 1/3\n",
            "19900/19900 [==============================] - 12s 599us/step - loss: 0.0552 - accuracy: 0.9834 - val_loss: 0.8272 - val_accuracy: 0.8000\n",
            "Epoch 2/3\n",
            "19900/19900 [==============================] - 12s 597us/step - loss: 0.0459 - accuracy: 0.9870 - val_loss: 0.8981 - val_accuracy: 0.7800\n",
            "Epoch 3/3\n",
            "19900/19900 [==============================] - 12s 597us/step - loss: 0.0389 - accuracy: 0.9892 - val_loss: 0.9291 - val_accuracy: 0.7800\n",
            "5000/5000 [==============================] - 1s 252us/step\n",
            "Accuracy: 0.7562000155448914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CA4Hr7arQCt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "5301faf1-ce84-4f97-9664-574e97cf3b6f"
      },
      "source": [
        "train_validate(3)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 19900 samples, validate on 100 samples\n",
            "Epoch 1/3\n",
            "19900/19900 [==============================] - 19s 945us/step - loss: 0.2839 - accuracy: 0.8698 - val_loss: 0.7888 - val_accuracy: 0.6500\n",
            "Epoch 2/3\n",
            "19900/19900 [==============================] - 19s 934us/step - loss: 0.2489 - accuracy: 0.8871 - val_loss: 0.8686 - val_accuracy: 0.6800\n",
            "Epoch 3/3\n",
            "19900/19900 [==============================] - 19s 935us/step - loss: 0.2196 - accuracy: 0.8983 - val_loss: 0.9662 - val_accuracy: 0.6300\n",
            "5000/5000 [==============================] - 2s 386us/step\n",
            "Accuracy: 0.647599995136261\n",
            "Train on 19900 samples, validate on 100 samples\n",
            "Epoch 1/3\n",
            "19900/19900 [==============================] - 19s 944us/step - loss: 0.1959 - accuracy: 0.9092 - val_loss: 1.0046 - val_accuracy: 0.6300\n",
            "Epoch 2/3\n",
            "19900/19900 [==============================] - 19s 940us/step - loss: 0.1777 - accuracy: 0.9169 - val_loss: 1.1131 - val_accuracy: 0.6200\n",
            "Epoch 3/3\n",
            "19900/19900 [==============================] - 19s 946us/step - loss: 0.1643 - accuracy: 0.9227 - val_loss: 1.1661 - val_accuracy: 0.6300\n",
            "5000/5000 [==============================] - 2s 406us/step\n",
            "Accuracy: 0.6424000263214111\n",
            "Train on 19900 samples, validate on 100 samples\n",
            "Epoch 1/3\n",
            "19900/19900 [==============================] - 19s 939us/step - loss: 0.1512 - accuracy: 0.9276 - val_loss: 1.2339 - val_accuracy: 0.6100\n",
            "Epoch 2/3\n",
            "19900/19900 [==============================] - 19s 938us/step - loss: 0.1405 - accuracy: 0.9319 - val_loss: 1.3344 - val_accuracy: 0.5900\n",
            "Epoch 3/3\n",
            "19900/19900 [==============================] - 19s 936us/step - loss: 0.1320 - accuracy: 0.9346 - val_loss: 1.3401 - val_accuracy: 0.6000\n",
            "5000/5000 [==============================] - 2s 319us/step\n",
            "Accuracy: 0.6388000249862671\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KtU4niAp8T7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df['lemmatized'], df['sentiment'], test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI0Wr4Pcc4gx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = CountVectorizer(binary=True, stop_words=stopwords.words('english'), lowercase=True, min_df=3, max_df=0.9, max_features=5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzGMDT0vdbbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def arr_to_padded(arr_train,arr_test):\n",
        "    vectorizer = CountVectorizer(binary=True, stop_words=stopwords.words('english'), lowercase=True, min_df=3, max_df=0.9, max_features=5000)\n",
        "    vectorizer.fit_transform(arr_train)\n",
        "    word2idx = {word: idx for idx, word in enumerate(vectorizer.get_feature_names())}\n",
        "    tokenizer = vectorizer.build_tokenizer()\n",
        "    preprocess = vectorizer.build_preprocessor()\n",
        "    \n",
        "    indexes_train = []    \n",
        "    for x in arr_train:\n",
        "        words = tokenizer(preprocess(x))\n",
        "        words_indexes = [word2idx[word] for word in words if word in word2idx]\n",
        "        indexes_train.append(words_indexes)\n",
        "    MAX_SEQ_LENGHT = len(max(indexes_train, key=len))  \n",
        "    N_FEATURES = len(vectorizer.get_feature_names())\n",
        "    padded_train = pad_sequences(indexes_train, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)\n",
        "    \n",
        "    indexes_test = []    \n",
        "    for x in arr_test:\n",
        "        words = tokenizer(preprocess(x))\n",
        "        words_indexes = [word2idx[word] for word in words if word in word2idx]\n",
        "        indexes_test.append(words_indexes)\n",
        "    padded_test = pad_sequences(indexes_test, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)    \n",
        "    \n",
        "    return padded_train,padded_test,vectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mIr9upTddyU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "padded_train,padded_test,vectorizer = arr_to_padded(X_train,X_test)\n",
        "MAX_SEQ_LENGHT = len(max(padded_train, key=len))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XiJ2LEfdgy6",
        "colab_type": "code",
        "outputId": "23b41972-076d-4edc-e31c-925733273367",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Embedding\n",
        " \n",
        "model = Sequential()\n",
        "model.add(Embedding(len(vectorizer.get_feature_names()) + 1,\n",
        "                    64,  # Embedding size\n",
        "                    input_length=MAX_SEQ_LENGHT))\n",
        "model.add(Conv1D(64, 5, activation='relu'))\n",
        "model.add(MaxPooling1D(5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        " \n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 263, 64)           238592    \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 259, 64)           20544     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 51, 64)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3264)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                208960    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 468,161\n",
            "Trainable params: 468,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWqJrUDtdr6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FalLR6c-d8bz",
        "colab_type": "code",
        "outputId": "95e44963-1755-4010-a526-dc674a5d154c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "train_validate(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 19900 samples, validate on 100 samples\n",
            "Epoch 1/3\n",
            "19900/19900 [==============================] - 29s 1ms/step - loss: 0.0857 - accuracy: 0.9767 - val_loss: 0.8459 - val_accuracy: 0.7600\n",
            "Epoch 2/3\n",
            "19900/19900 [==============================] - 28s 1ms/step - loss: 0.0678 - accuracy: 0.9833 - val_loss: 0.9348 - val_accuracy: 0.7700\n",
            "Epoch 3/3\n",
            "19900/19900 [==============================] - 28s 1ms/step - loss: 0.0552 - accuracy: 0.9873 - val_loss: 1.0253 - val_accuracy: 0.7600\n",
            "5000/5000 [==============================] - 2s 474us/step\n",
            "Accuracy: 0.7788000106811523\n",
            "Train on 19900 samples, validate on 100 samples\n",
            "Epoch 1/3\n",
            "19900/19900 [==============================] - 28s 1ms/step - loss: 0.0447 - accuracy: 0.9900 - val_loss: 1.0770 - val_accuracy: 0.7500\n",
            "Epoch 2/3\n",
            "19900/19900 [==============================] - 28s 1ms/step - loss: 0.0378 - accuracy: 0.9923 - val_loss: 1.1674 - val_accuracy: 0.7600\n",
            "Epoch 3/3\n",
            "19900/19900 [==============================] - 28s 1ms/step - loss: 0.0311 - accuracy: 0.9937 - val_loss: 1.2289 - val_accuracy: 0.7500\n",
            "5000/5000 [==============================] - 2s 434us/step\n",
            "Accuracy: 0.7731999754905701\n",
            "Train on 19900 samples, validate on 100 samples\n",
            "Epoch 1/3\n",
            "19900/19900 [==============================] - 28s 1ms/step - loss: 0.0264 - accuracy: 0.9948 - val_loss: 1.3029 - val_accuracy: 0.7600\n",
            "Epoch 2/3\n",
            "19900/19900 [==============================] - 29s 1ms/step - loss: 0.0227 - accuracy: 0.9953 - val_loss: 1.3619 - val_accuracy: 0.7800\n",
            "Epoch 3/3\n",
            "19900/19900 [==============================] - 28s 1ms/step - loss: 0.0196 - accuracy: 0.9958 - val_loss: 1.4299 - val_accuracy: 0.7600\n",
            "5000/5000 [==============================] - 2s 413us/step\n",
            "Accuracy: 0.7666000127792358\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8f27OBld-XG",
        "colab_type": "code",
        "outputId": "75db9cf0-d3d1-475c-a08d-6d5c72189563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "model.fit(padded_train[:-100], y_train[:-100], \n",
        "          epochs=3, batch_size=512, verbose=1,\n",
        "          validation_data=(padded_train[-100:], y_train[-100:]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 19900 samples, validate on 100 samples\n",
            "Epoch 1/3\n",
            "19900/19900 [==============================] - 24s 1ms/step - loss: 0.3207 - accuracy: 0.8667 - val_loss: 0.4331 - val_accuracy: 0.7600\n",
            "Epoch 2/3\n",
            "19900/19900 [==============================] - 24s 1ms/step - loss: 0.2877 - accuracy: 0.8839 - val_loss: 0.4774 - val_accuracy: 0.7700\n",
            "Epoch 3/3\n",
            "19900/19900 [==============================] - 24s 1ms/step - loss: 0.2580 - accuracy: 0.8982 - val_loss: 0.4808 - val_accuracy: 0.7800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fcae10a5da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpW8w9RKeQ9L",
        "colab_type": "code",
        "outputId": "e31ee2e8-b1d3-4ed9-f177-a10be5abd751",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "scores = model.evaluate(padded_test, y_test, verbose=1)\n",
        "print(\"Accuracy:\", scores[1]) # 0.875"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000/5000 [==============================] - 2s 355us/step\n",
            "Accuracy: 0.7865999937057495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1L44MpKeSVh",
        "colab_type": "code",
        "outputId": "9b52fc4c-9a72-4cc9-a56a-7e4f1d571507",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "model.fit(padded_train[:-100], y_train[:-100], \n",
        "          epochs=3, batch_size=512, verbose=1,\n",
        "          validation_data=(padded_train[-100:], y_train[-100:]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 19900 samples, validate on 100 samples\n",
            "Epoch 1/3\n",
            "19900/19900 [==============================] - 24s 1ms/step - loss: 0.2214 - accuracy: 0.9155 - val_loss: 0.5586 - val_accuracy: 0.7600\n",
            "Epoch 2/3\n",
            "19900/19900 [==============================] - 24s 1ms/step - loss: 0.1863 - accuracy: 0.9340 - val_loss: 0.6178 - val_accuracy: 0.7400\n",
            "Epoch 3/3\n",
            "19900/19900 [==============================] - 24s 1ms/step - loss: 0.1512 - accuracy: 0.9498 - val_loss: 0.6668 - val_accuracy: 0.7500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fcae12e3668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGfGwW-me0tw",
        "colab_type": "code",
        "outputId": "e9d9cbca-2698-4db5-8bf2-33272af9c034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "scores = model.evaluate(padded_test, y_test, verbose=1)\n",
        "print(\"Accuracy:\", scores[1]) # 0.875"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000/5000 [==============================] - 2s 372us/step\n",
            "Accuracy: 0.7796000242233276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZNU8vmae2RK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}