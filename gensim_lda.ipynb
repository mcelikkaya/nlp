{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gensim_lda.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PORGKiMRK20l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "In this notebook , I try LDA(Latent Dirichlet Allocation) \n",
        "for content downloaded from wikipedia.\n",
        "I am downloading prefectures of Japan and states of USA.\n",
        "Then give these vocublary to LDA and ask to make 2 topics.\n",
        "\"\"\"\n",
        "import gensim\n",
        "from gensim import corpora"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npztCCFmLJww",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "a767d20f-564b-4fa0-c8d9-dd00253d10cc"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzIyZYaqK36W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wikipedia\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "import string\n",
        "stop = set(stopwords.words('english'))\n",
        "exclude = set(string.punctuation)\n",
        "lemma = WordNetLemmatizer()\n",
        "def clean(doc):\n",
        "  stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
        "  punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
        "  normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
        "  return normalized\n",
        "\n",
        "def download_from_wiki(keywords):\n",
        "  splits = keywords.split(\",\")\n",
        "  sentences = []\n",
        "  for split in splits:\n",
        "    try:\n",
        "      sentences.append(wikipedia.summary( split ))\n",
        "    except:\n",
        "      print(\"error \",split) \n",
        "  return sentences  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl_T9rzJK9uy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "2504b795-2c4d-45a2-f0fa-89071b295d9f"
      },
      "source": [
        "japan_states = \"Hokkaidō,Aomori,Iwate,Miyagi,Akita,Yamagata,Fukushima,Ibaraki,Tochigi,Gunma,Saitama,Chiba,Tōkyō,Kanagawa,Niigata,Toyama,Ishikawa,Fukui,Yamanashi,Nagano,Gifu,Shizuoka,Aichi,Mie,Shiga,Kyōto,Ōsaka,Hyōgo,Nara,Wakayama,Tottori,Shimane,Okayama,Hiroshima,Yamaguchi,Tokushima,Kagawa,Ehime,Kōchi,Fukuoka,Saga,Nagasaki,Kumamoto,Ōita,Miyazaki,Kagoshima,Okinawa\"\n",
        "\n",
        "japan_sentences = download_from_wiki( japan_states )\n",
        "\n",
        "usa_states = \"Alabama,Alaska,Arizona,Arkansas,California,Colorado,Connecticut,Delaware,Florida,Georgia,Hawaii,Idaho,Illinois,Indiana,Iowa,Kansas,Kentucky,Louisiana,Maine,Maryland,Massachusetts,Michigan,Minnesota,Mississippi,Missouri,Montana,Nebraska,Nevada,New Hampshire,New Jersey,New Mexico,New York,North Carolina,North Dakota,Ohio,Oklahoma,Oregon,Pennsylvania,Rhode Island,South Carolina,South Dakota,Tennessee,Texas,Utah,Vermont,Virginia,Washington,West Virginia,Wisconsin,Wyoming\"\n",
        "usa_sentences = download_from_wiki( usa_states )"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.6/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error  Miyagi\n",
            "error  Fukushima\n",
            "error  Tochigi\n",
            "error  Saitama\n",
            "error  Niigata\n",
            "error  Toyama\n",
            "error  Ishikawa\n",
            "error  Fukui\n",
            "error  Yamanashi\n",
            "error  Nagano\n",
            "error  Shizuoka\n",
            "error  Mie\n",
            "error  Nara\n",
            "error  Wakayama\n",
            "error  Yamaguchi\n",
            "error  Georgia\n",
            "error  Indiana\n",
            "error  New York\n",
            "error  Rhode Island\n",
            "error  Washington\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAMT9VzqK_Ap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_clean_japan = [clean(doc).split() for doc in japan_sentences]\n",
        "doc_clean_usa = [clean(doc).split() for doc in usa_sentences]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9soNz5WLBMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def run_lda(vocublary,num_topic,num_pass):\n",
        "  dictionary = Dictionary(vocublary)\n",
        "  doc_term_matrix = [dictionary.doc2bow(doc) for doc in vocublary]\n",
        "  Lda = gensim.models.ldamodel.LdaModel\n",
        "  model = Lda(doc_term_matrix, num_topics=num_topic, id2word = dictionary, passes=num_pass)\n",
        "  #print(model.print_topics(num_topics=num_topic, num_words=6))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQfq5Ky_djIN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "50b0e6bd-a1f8-409f-d60a-34ac79b5c5b4"
      },
      "source": [
        "model1 = run_lda(doc_clean_usa,2)\n",
        "model1.print_topics(-1)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.043*\"state\" + 0.010*\"area\" + 0.008*\"u\" + 0.008*\"united\" + 0.007*\"largest\" + 0.007*\"city\" + 0.006*\"south\" + 0.006*\"american\" + 0.006*\"new\" + 0.005*\"populous\"'),\n",
              " (1,\n",
              "  '0.041*\"state\" + 0.011*\"new\" + 0.010*\"united\" + 0.008*\"north\" + 0.007*\"area\" + 0.006*\"mexico\" + 0.006*\"city\" + 0.006*\"virginia\" + 0.006*\"largest\" + 0.006*\"west\"')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-WRzA6SdlUq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "252cf4d0-e4e0-43af-d10e-abdca6fa1825"
      },
      "source": [
        "model1 = run_lda(doc_clean_usa+doc_clean_japan,2,500)\n",
        "model1.print_topics(-1)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.027*\"prefecture\" + 0.018*\"city\" + 0.017*\"japan\" + 0.008*\"area\" + 0.007*\"capital\" + 0.007*\"population\" + 0.006*\"largest\" + 0.006*\"world\" + 0.006*\"region\" + 0.005*\"japanese\"'),\n",
              " (1,\n",
              "  '0.046*\"state\" + 0.009*\"area\" + 0.009*\"united\" + 0.008*\"new\" + 0.007*\"largest\" + 0.007*\"north\" + 0.007*\"city\" + 0.007*\"u\" + 0.006*\"american\" + 0.006*\"south\"')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgN-5rX5T1oG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "ec3b54e5-2d34-4aa5-9850-03038da00319"
      },
      "source": [
        "model1 = run_lda(doc_clean_usa+doc_clean_japan,2,3000)\n",
        "model1.print_topics(-1)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.005*\"china\" + 0.005*\"missouri\" + 0.005*\"world\" + 0.005*\"film\" + 0.004*\"kansa\" + 0.004*\"japanese\" + 0.003*\"massachusetts\" + 0.003*\"akita\" + 0.003*\"japan\" + 0.003*\"osaka\"'),\n",
              " (1,\n",
              "  '0.041*\"state\" + 0.013*\"city\" + 0.011*\"area\" + 0.010*\"prefecture\" + 0.008*\"largest\" + 0.008*\"united\" + 0.007*\"new\" + 0.007*\"population\" + 0.007*\"north\" + 0.007*\"region\"')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_f64GrzT20_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "ebfae8fd-9e3d-4d48-b4d3-f388cf1b839d"
      },
      "source": [
        "#as the number of pass increases keywords get less importance and more refined\n",
        "model1 = run_lda(doc_clean_usa+doc_clean_japan,2,5000)\n",
        "model1.print_topics(-1)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.045*\"state\" + 0.009*\"area\" + 0.009*\"united\" + 0.008*\"new\" + 0.007*\"largest\" + 0.007*\"north\" + 0.007*\"city\" + 0.007*\"u\" + 0.006*\"american\" + 0.006*\"south\"'),\n",
              " (1,\n",
              "  '0.028*\"prefecture\" + 0.019*\"city\" + 0.018*\"japan\" + 0.008*\"area\" + 0.007*\"capital\" + 0.007*\"population\" + 0.006*\"japanese\" + 0.006*\"largest\" + 0.006*\"region\" + 0.005*\"located\"')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkvsHCZ0fzqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}